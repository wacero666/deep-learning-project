{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout, Activation, LSTM, Lambda\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Reshape\n",
    "from keras.models import Sequential\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Reshape\n",
    "from keras.layers.merge import concatenate, dot\n",
    "from keras.layers.core import Reshape\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "def neg_sensitivity(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return 1 - true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x1a447684a8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model():\n",
    "    kernel_size = 5\n",
    "    filters = 5\n",
    "    stride_width = 1\n",
    "    stirde_height = 3\n",
    "    pool_size = 4\n",
    "    \n",
    "    inp = Input(shape = (2, 64, 120, 3))\n",
    "    mat1 = Lambda(lambda x: x[:, 0])(inp)\n",
    "    mat2 = Lambda(lambda x: x[:, 1])(inp)\n",
    "    mat1 = Convolution2D(filters = 10, kernel_size = 5, \n",
    "                                         strides = (2, 5), activation = 'relu')(mat1)\n",
    "    mat2 = Convolution2D(filters = 10, kernel_size = 5, \n",
    "                                         strides = (2, 5), activation = 'relu')(mat2)\n",
    "    mat1 = MaxPooling2D(pool_size = 2, strides = 2)(mat1)\n",
    "    mat2 = MaxPooling2D(pool_size = 2, strides = 2)(mat2)\n",
    "    mat1 = Convolution2D(filters = 5, kernel_size = 2, \n",
    "                                         strides = (1, 1), activation = 'relu')(mat1)\n",
    "    mat2 = Convolution2D(filters = 5, kernel_size = 2, \n",
    "                                         strides = (1, 1), activation = 'relu')(mat2)\n",
    "    \n",
    "    mat1 = MaxPooling2D(pool_size = 2, strides = 2)(mat1)\n",
    "    mat2 = MaxPooling2D(pool_size = 2, strides = 2)(mat2)\n",
    "    #mat1 = TimeDistributed(Dropout(rate = 0.25))(mat1)\n",
    "    #mat2 = TimeDistributed(Dropout(rate = 0.25))(mat2)\n",
    "    mat1 = Convolution2D(filters = 3, kernel_size = 2, \n",
    "                                         strides = (1, 1), activation = 'relu')(mat1)\n",
    "    mat2 = Convolution2D(filters = 3, kernel_size = 2, \n",
    "                                         strides = (1, 1), activation = 'relu')(mat2)\n",
    "    mat1 = MaxPooling2D(pool_size = 2, strides = 2)(mat1)\n",
    "    mat2 = MaxPooling2D(pool_size = 2, strides = 2)(mat2)\n",
    "    mat1 = Flatten()(mat1)\n",
    "    mat2 = Flatten()(mat2)\n",
    "    \n",
    "    vec3 = dot([mat1, mat2], axes = 1)\n",
    "    vec4 = concatenate([mat1, mat2, vec3])\n",
    "\n",
    "    vec4 = Dense(100,  activation='relu')(vec4)\n",
    "    #vec4 = Dropout(rate=0.25)(vec4)\n",
    "    preds = Dense(1, activation='sigmoid')(vec4)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=preds)\n",
    "    \n",
    "    opt = Adam(lr=0.001, decay=1e-6)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics =[sensitivity, 'acc'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.load('train_set_score.npy')\n",
    "\n",
    "val_set = np.load('val_set_score.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1630, 3)\n",
      "118\n",
      "1400\n"
     ]
    }
   ],
   "source": [
    "val_set_bin = np.vectorize(lambda x: int(x))(val_set[:, 2] > 0.5)\n",
    "idx_1 = np.where(val_set_bin == 1)\n",
    "idx_1 = np.array(idx_1).reshape(len(idx_1[0]),)\n",
    "idx_0 = np.where(val_set_bin == 0)\n",
    "idx_0 = np.random.choice(np.array(idx_0).reshape(len(idx_0[0]),), np.sum(val_set_bin), replace = False)\n",
    "val_set = val_set[np.append(idx_1, idx_0), :]\n",
    "print(val_set.shape)\n",
    "\n",
    "\n",
    "train_set_bin = np.vectorize(lambda x: int(x))(train_set[6000:7400, 2] > 0.5)\n",
    "print(np.sum(train_set_bin))\n",
    "print(len(train_set_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00537116]\n",
      " [0.0159039]\n",
      " [0.00853313]\n",
      " ...\n",
      " [1.0]\n",
      " [0.00160056]\n",
      " [0.0276892]]\n",
      "[0.60374116 0.59831096 0.60194721 ... 8.10391276 0.60606684 0.59405223]\n"
     ]
    }
   ],
   "source": [
    "y = train_set[:6000, 2]\n",
    "y = y.reshape(-1, 1)\n",
    "print(y)\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(y)\n",
    "score = kde.score_samples(y)\n",
    "weight = 1/np.exp(score).reshape(-1,)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 120, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 2, 64, 120, 3)\n",
      "(6000,)\n",
      "(1400, 2, 64, 120, 3)\n",
      "(1400,)\n",
      "(1630, 2, 64, 120, 3)\n",
      "(1630,)\n"
     ]
    }
   ],
   "source": [
    "n_steps = 20 #10seconds\n",
    "n_lap = 10 #5seconds\n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "for i in range(6000):\n",
    "    image1 = train_set[i][0]\n",
    "    image2 = train_set[i][1]\n",
    "    y = int(train_set[i][2] > 0.5)\n",
    "    #y = train_set[i][2]\n",
    "    train_x.append(np.array([image1, image2]))\n",
    "    train_y.append(y)\n",
    "    \n",
    "train_x = np.array(train_x)\n",
    "print(train_x.shape)\n",
    "train_y = np.array(train_y)\n",
    "print(train_y.shape)\n",
    "\n",
    "validation_x = []\n",
    "validation_y = []\n",
    "for i in range(6000, 7400):\n",
    "    image1 = train_set[i][0]\n",
    "    image2 = train_set[i][1]\n",
    "    y = int(train_set[i][2] > 0.5)\n",
    "    #y = train_set[i][2]\n",
    "    validation_x.append(np.array([image1, image2]))\n",
    "    validation_y.append(y)\n",
    "    \n",
    "validation_x = np.array(validation_x)\n",
    "print(validation_x.shape)\n",
    "validation_y = np.array(validation_y)\n",
    "print(validation_y.shape)\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "for i in range(len(val_set)):\n",
    "    image1 = val_set[i][0]\n",
    "    image2 = val_set[i][1]\n",
    "    y = int(val_set[i][2] > 0.5)\n",
    "    #y = val_set[i][2]\n",
    "    test_x.append(np.array([image1, image2]))\n",
    "    test_y.append(y)\n",
    "    \n",
    "test_x = np.array(test_x)\n",
    "print(test_x.shape)\n",
    "test_y = np.array(test_y)\n",
    "print(test_y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1630 samples\n",
      "Epoch 1/200\n",
      "6000/6000 [==============================] - 5s 778us/step - loss: 1.9962 - sensitivity: 0.5422 - acc: 0.4937 - val_loss: 0.7102 - val_sensitivity: 0.2526 - val_acc: 0.5184\n",
      "Epoch 2/200\n",
      "6000/6000 [==============================] - 3s 490us/step - loss: 1.2673 - sensitivity: 0.4826 - acc: 0.5290 - val_loss: 0.6943 - val_sensitivity: 0.1838 - val_acc: 0.5160\n",
      "Epoch 3/200\n",
      "6000/6000 [==============================] - 3s 502us/step - loss: 1.2506 - sensitivity: 0.5506 - acc: 0.5215 - val_loss: 0.7007 - val_sensitivity: 0.3272 - val_acc: 0.5209\n",
      "Epoch 4/200\n",
      "6000/6000 [==============================] - 3s 505us/step - loss: 1.2478 - sensitivity: 0.4796 - acc: 0.6008 - val_loss: 0.6925 - val_sensitivity: 0.2196 - val_acc: 0.5294\n",
      "Epoch 5/200\n",
      "6000/6000 [==============================] - 3s 496us/step - loss: 1.2419 - sensitivity: 0.5713 - acc: 0.5527 - val_loss: 0.6910 - val_sensitivity: 0.3519 - val_acc: 0.5380\n",
      "Epoch 6/200\n",
      "6000/6000 [==============================] - 3s 514us/step - loss: 1.2450 - sensitivity: 0.6366 - acc: 0.5035 - val_loss: 0.6874 - val_sensitivity: 0.3301 - val_acc: 0.5411\n",
      "Epoch 7/200\n",
      "6000/6000 [==============================] - 3s 512us/step - loss: 1.2370 - sensitivity: 0.5695 - acc: 0.5977 - val_loss: 0.6848 - val_sensitivity: 0.4115 - val_acc: 0.5528\n",
      "Epoch 8/200\n",
      "6000/6000 [==============================] - 3s 503us/step - loss: 1.2220 - sensitivity: 0.6189 - acc: 0.5575 - val_loss: 0.6760 - val_sensitivity: 0.3894 - val_acc: 0.5828\n",
      "Epoch 9/200\n",
      "6000/6000 [==============================] - 3s 528us/step - loss: 1.2133 - sensitivity: 0.6727 - acc: 0.5350 - val_loss: 0.6755 - val_sensitivity: 0.2378 - val_acc: 0.5804\n",
      "Epoch 10/200\n",
      "6000/6000 [==============================] - 3s 531us/step - loss: 1.2081 - sensitivity: 0.5720 - acc: 0.6173 - val_loss: 0.6662 - val_sensitivity: 0.3213 - val_acc: 0.6086\n",
      "Epoch 11/200\n",
      "6000/6000 [==============================] - 3s 501us/step - loss: 1.1788 - sensitivity: 0.6352 - acc: 0.6108 - val_loss: 0.6550 - val_sensitivity: 0.3798 - val_acc: 0.6350\n",
      "Epoch 12/200\n",
      "6000/6000 [==============================] - 3s 524us/step - loss: 1.1646 - sensitivity: 0.6279 - acc: 0.6452 - val_loss: 0.6551 - val_sensitivity: 0.3900 - val_acc: 0.6123\n",
      "Epoch 13/200\n",
      "6000/6000 [==============================] - 3s 526us/step - loss: 1.1649 - sensitivity: 0.6336 - acc: 0.6100 - val_loss: 0.6506 - val_sensitivity: 0.3902 - val_acc: 0.6344\n",
      "Epoch 14/200\n",
      "6000/6000 [==============================] - 3s 511us/step - loss: 1.1452 - sensitivity: 0.6604 - acc: 0.6083 - val_loss: 0.6510 - val_sensitivity: 0.3256 - val_acc: 0.6196\n",
      "Epoch 15/200\n",
      "6000/6000 [==============================] - 3s 500us/step - loss: 1.1204 - sensitivity: 0.6521 - acc: 0.6538 - val_loss: 0.6497 - val_sensitivity: 0.4123 - val_acc: 0.6276\n",
      "Epoch 16/200\n",
      "6000/6000 [==============================] - 3s 501us/step - loss: 1.1440 - sensitivity: 0.6457 - acc: 0.6385 - val_loss: 0.6528 - val_sensitivity: 0.2726 - val_acc: 0.6196\n",
      "Epoch 17/200\n",
      "6000/6000 [==============================] - 3s 508us/step - loss: 1.0851 - sensitivity: 0.6699 - acc: 0.6542 - val_loss: 0.6345 - val_sensitivity: 0.3460 - val_acc: 0.6583\n",
      "Epoch 18/200\n",
      "6000/6000 [==============================] - 3s 511us/step - loss: 1.0807 - sensitivity: 0.6522 - acc: 0.6715 - val_loss: 0.6290 - val_sensitivity: 0.3996 - val_acc: 0.6577\n",
      "Epoch 19/200\n",
      "6000/6000 [==============================] - 3s 509us/step - loss: 1.0655 - sensitivity: 0.7081 - acc: 0.6492 - val_loss: 0.6406 - val_sensitivity: 0.3740 - val_acc: 0.6485\n",
      "Epoch 20/200\n",
      "6000/6000 [==============================] - 3s 530us/step - loss: 1.0620 - sensitivity: 0.7110 - acc: 0.6520 - val_loss: 0.6352 - val_sensitivity: 0.3227 - val_acc: 0.6644\n",
      "Epoch 21/200\n",
      "6000/6000 [==============================] - 3s 528us/step - loss: 1.0533 - sensitivity: 0.7111 - acc: 0.6900 - val_loss: 0.6313 - val_sensitivity: 0.3344 - val_acc: 0.6650\n",
      "Epoch 22/200\n",
      "6000/6000 [==============================] - 3s 520us/step - loss: 1.0251 - sensitivity: 0.7099 - acc: 0.6870 - val_loss: 0.6259 - val_sensitivity: 0.3916 - val_acc: 0.6681\n",
      "Epoch 23/200\n",
      "6000/6000 [==============================] - 3s 514us/step - loss: 1.0240 - sensitivity: 0.7135 - acc: 0.6817 - val_loss: 0.6433 - val_sensitivity: 0.3546 - val_acc: 0.6669\n",
      "Epoch 24/200\n",
      "6000/6000 [==============================] - 3s 501us/step - loss: 1.0061 - sensitivity: 0.7036 - acc: 0.7060 - val_loss: 0.6284 - val_sensitivity: 0.3851 - val_acc: 0.6663\n",
      "Epoch 25/200\n",
      "6000/6000 [==============================] - 3s 501us/step - loss: 1.0055 - sensitivity: 0.7410 - acc: 0.6905 - val_loss: 0.6361 - val_sensitivity: 0.3204 - val_acc: 0.6687\n",
      "Epoch 26/200\n",
      "6000/6000 [==============================] - 3s 507us/step - loss: 0.9883 - sensitivity: 0.7427 - acc: 0.6973 - val_loss: 0.6263 - val_sensitivity: 0.3372 - val_acc: 0.6669\n",
      "Epoch 27/200\n",
      "6000/6000 [==============================] - 3s 509us/step - loss: 0.9589 - sensitivity: 0.7604 - acc: 0.7055 - val_loss: 0.6473 - val_sensitivity: 0.3611 - val_acc: 0.6865\n",
      "Epoch 28/200\n",
      "6000/6000 [==============================] - 3s 508us/step - loss: 0.9800 - sensitivity: 0.7674 - acc: 0.6943 - val_loss: 0.6234 - val_sensitivity: 0.3393 - val_acc: 0.6840\n",
      "Epoch 29/200\n",
      "6000/6000 [==============================] - 3s 510us/step - loss: 0.9551 - sensitivity: 0.7472 - acc: 0.7042 - val_loss: 0.6264 - val_sensitivity: 0.4022 - val_acc: 0.6890\n",
      "Epoch 30/200\n",
      "6000/6000 [==============================] - 3s 509us/step - loss: 0.9367 - sensitivity: 0.7905 - acc: 0.7092 - val_loss: 0.7433 - val_sensitivity: 0.2002 - val_acc: 0.6258\n",
      "Epoch 31/200\n",
      "6000/6000 [==============================] - 3s 510us/step - loss: 0.9468 - sensitivity: 0.7678 - acc: 0.7177 - val_loss: 0.6203 - val_sensitivity: 0.3953 - val_acc: 0.6773\n",
      "Epoch 32/200\n",
      "6000/6000 [==============================] - 3s 506us/step - loss: 0.9064 - sensitivity: 0.7797 - acc: 0.7370 - val_loss: 0.6502 - val_sensitivity: 0.4119 - val_acc: 0.6926\n",
      "Epoch 33/200\n",
      "6000/6000 [==============================] - 3s 509us/step - loss: 0.9066 - sensitivity: 0.7758 - acc: 0.7242 - val_loss: 0.6236 - val_sensitivity: 0.3967 - val_acc: 0.6840\n",
      "Epoch 34/200\n",
      "6000/6000 [==============================] - 3s 559us/step - loss: 0.8920 - sensitivity: 0.8101 - acc: 0.7212 - val_loss: 0.6150 - val_sensitivity: 0.3703 - val_acc: 0.6920\n",
      "Epoch 35/200\n",
      "6000/6000 [==============================] - 3s 582us/step - loss: 0.9231 - sensitivity: 0.7869 - acc: 0.7125 - val_loss: 0.6324 - val_sensitivity: 0.3544 - val_acc: 0.6810\n",
      "Epoch 36/200\n",
      "6000/6000 [==============================] - 3s 548us/step - loss: 0.8761 - sensitivity: 0.8300 - acc: 0.7187 - val_loss: 0.6293 - val_sensitivity: 0.3708 - val_acc: 0.7110\n",
      "Epoch 37/200\n",
      "6000/6000 [==============================] - 3s 519us/step - loss: 0.8736 - sensitivity: 0.7903 - acc: 0.7288 - val_loss: 0.6257 - val_sensitivity: 0.3573 - val_acc: 0.7067\n",
      "Epoch 38/200\n",
      "6000/6000 [==============================] - 3s 524us/step - loss: 0.8994 - sensitivity: 0.7832 - acc: 0.7222 - val_loss: 0.6642 - val_sensitivity: 0.3634 - val_acc: 0.6982\n",
      "Epoch 39/200\n",
      "6000/6000 [==============================] - 3s 532us/step - loss: 0.8709 - sensitivity: 0.8046 - acc: 0.7230 - val_loss: 0.6120 - val_sensitivity: 0.4082 - val_acc: 0.7233\n",
      "Epoch 40/200\n",
      "6000/6000 [==============================] - 3s 496us/step - loss: 0.8302 - sensitivity: 0.8284 - acc: 0.7358 - val_loss: 0.6219 - val_sensitivity: 0.3924 - val_acc: 0.7135\n",
      "Epoch 41/200\n",
      "6000/6000 [==============================] - 3s 498us/step - loss: 0.8418 - sensitivity: 0.8240 - acc: 0.7353 - val_loss: 0.6478 - val_sensitivity: 0.3777 - val_acc: 0.7141\n",
      "Epoch 42/200\n",
      "6000/6000 [==============================] - 3s 543us/step - loss: 0.8226 - sensitivity: 0.8291 - acc: 0.7477 - val_loss: 0.6237 - val_sensitivity: 0.4407 - val_acc: 0.7147\n",
      "Epoch 43/200\n",
      "6000/6000 [==============================] - 3s 497us/step - loss: 0.8129 - sensitivity: 0.8588 - acc: 0.7462 - val_loss: 0.6393 - val_sensitivity: 0.3973 - val_acc: 0.7227\n",
      "Epoch 44/200\n",
      "6000/6000 [==============================] - 3s 499us/step - loss: 0.7862 - sensitivity: 0.8432 - acc: 0.7533 - val_loss: 0.6411 - val_sensitivity: 0.3638 - val_acc: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/200\n",
      "6000/6000 [==============================] - 3s 486us/step - loss: 0.8107 - sensitivity: 0.8601 - acc: 0.7433 - val_loss: 0.6703 - val_sensitivity: 0.3863 - val_acc: 0.7252\n",
      "Epoch 46/200\n",
      "6000/6000 [==============================] - 3s 484us/step - loss: 0.8177 - sensitivity: 0.8258 - acc: 0.7463 - val_loss: 0.6239 - val_sensitivity: 0.3906 - val_acc: 0.7374\n",
      "Epoch 47/200\n",
      "6000/6000 [==============================] - 3s 486us/step - loss: 0.8275 - sensitivity: 0.8279 - acc: 0.7443 - val_loss: 0.6414 - val_sensitivity: 0.3847 - val_acc: 0.7325\n",
      "Epoch 48/200\n",
      "6000/6000 [==============================] - 3s 541us/step - loss: 0.7511 - sensitivity: 0.8561 - acc: 0.7628 - val_loss: 0.6326 - val_sensitivity: 0.4051 - val_acc: 0.7282\n",
      "Epoch 49/200\n",
      "6000/6000 [==============================] - 3s 517us/step - loss: 0.7527 - sensitivity: 0.8920 - acc: 0.7600 - val_loss: 0.6586 - val_sensitivity: 0.4020 - val_acc: 0.7380\n",
      "Epoch 50/200\n",
      "6000/6000 [==============================] - 3s 506us/step - loss: 0.7642 - sensitivity: 0.8526 - acc: 0.7660 - val_loss: 0.6336 - val_sensitivity: 0.4331 - val_acc: 0.7411\n",
      "Epoch 51/200\n",
      "6000/6000 [==============================] - 3s 515us/step - loss: 0.7265 - sensitivity: 0.8861 - acc: 0.7727 - val_loss: 0.6356 - val_sensitivity: 0.4143 - val_acc: 0.7368\n",
      "Epoch 52/200\n",
      "6000/6000 [==============================] - 3s 525us/step - loss: 0.7309 - sensitivity: 0.8722 - acc: 0.7750 - val_loss: 0.6419 - val_sensitivity: 0.4135 - val_acc: 0.7454\n",
      "Epoch 53/200\n",
      "6000/6000 [==============================] - 3s 518us/step - loss: 0.7015 - sensitivity: 0.8892 - acc: 0.7785 - val_loss: 0.6425 - val_sensitivity: 0.4072 - val_acc: 0.7454\n",
      "Epoch 54/200\n",
      "6000/6000 [==============================] - 3s 520us/step - loss: 0.6978 - sensitivity: 0.8742 - acc: 0.7817 - val_loss: 0.6518 - val_sensitivity: 0.4133 - val_acc: 0.7552\n",
      "Epoch 55/200\n",
      "6000/6000 [==============================] - 3s 504us/step - loss: 0.8024 - sensitivity: 0.8535 - acc: 0.7403 - val_loss: 0.6388 - val_sensitivity: 0.3994 - val_acc: 0.7540\n",
      "Epoch 56/200\n",
      "6000/6000 [==============================] - 3s 501us/step - loss: 0.7026 - sensitivity: 0.8842 - acc: 0.7800 - val_loss: 0.6418 - val_sensitivity: 0.4065 - val_acc: 0.7656\n",
      "Epoch 57/200\n",
      "6000/6000 [==============================] - 3s 493us/step - loss: 0.6903 - sensitivity: 0.8678 - acc: 0.7862 - val_loss: 0.6602 - val_sensitivity: 0.4170 - val_acc: 0.7429\n",
      "Epoch 58/200\n",
      "6000/6000 [==============================] - 3s 503us/step - loss: 0.6689 - sensitivity: 0.8974 - acc: 0.7880 - val_loss: 0.6733 - val_sensitivity: 0.3928 - val_acc: 0.7613\n",
      "Epoch 59/200\n",
      "6000/6000 [==============================] - 3s 500us/step - loss: 0.6682 - sensitivity: 0.8934 - acc: 0.7945 - val_loss: 0.6713 - val_sensitivity: 0.4194 - val_acc: 0.7356\n",
      "Epoch 60/200\n",
      "6000/6000 [==============================] - 3s 503us/step - loss: 0.6945 - sensitivity: 0.8618 - acc: 0.7870 - val_loss: 0.6313 - val_sensitivity: 0.4352 - val_acc: 0.7736\n",
      "Epoch 61/200\n",
      "6000/6000 [==============================] - 3s 503us/step - loss: 0.6634 - sensitivity: 0.9033 - acc: 0.7885 - val_loss: 0.6570 - val_sensitivity: 0.4086 - val_acc: 0.7595\n",
      "Epoch 62/200\n",
      "6000/6000 [==============================] - 3s 504us/step - loss: 0.6585 - sensitivity: 0.8990 - acc: 0.7973 - val_loss: 0.6512 - val_sensitivity: 0.4174 - val_acc: 0.7546\n",
      "Epoch 63/200\n",
      "6000/6000 [==============================] - 3s 504us/step - loss: 0.6646 - sensitivity: 0.8971 - acc: 0.7857 - val_loss: 0.6530 - val_sensitivity: 0.4231 - val_acc: 0.7718\n",
      "Epoch 64/200\n",
      "6000/6000 [==============================] - 3s 504us/step - loss: 0.6606 - sensitivity: 0.8911 - acc: 0.8023 - val_loss: 0.6767 - val_sensitivity: 0.4057 - val_acc: 0.7509\n",
      "Epoch 65/200\n",
      "6000/6000 [==============================] - 3s 510us/step - loss: 0.6659 - sensitivity: 0.8957 - acc: 0.7962 - val_loss: 0.6527 - val_sensitivity: 0.4427 - val_acc: 0.7411\n",
      "Epoch 66/200\n",
      "6000/6000 [==============================] - 3s 508us/step - loss: 0.6989 - sensitivity: 0.8726 - acc: 0.7767 - val_loss: 0.6547 - val_sensitivity: 0.4315 - val_acc: 0.7669\n",
      "Epoch 67/200\n",
      "6000/6000 [==============================] - 3s 499us/step - loss: 0.6383 - sensitivity: 0.8968 - acc: 0.7993 - val_loss: 0.7165 - val_sensitivity: 0.3900 - val_acc: 0.7368\n",
      "Epoch 68/200\n",
      "6000/6000 [==============================] - 3s 501us/step - loss: 0.6776 - sensitivity: 0.8706 - acc: 0.7953 - val_loss: 0.6758 - val_sensitivity: 0.4055 - val_acc: 0.7748\n",
      "Epoch 69/200\n",
      "6000/6000 [==============================] - 3s 506us/step - loss: 0.5888 - sensitivity: 0.9217 - acc: 0.8170 - val_loss: 0.6600 - val_sensitivity: 0.4202 - val_acc: 0.7761\n",
      "Epoch 70/200\n",
      "6000/6000 [==============================] - 3s 503us/step - loss: 0.5707 - sensitivity: 0.9234 - acc: 0.8245 - val_loss: 0.6878 - val_sensitivity: 0.4190 - val_acc: 0.7779\n",
      "Epoch 71/200\n",
      "6000/6000 [==============================] - 3s 502us/step - loss: 0.6144 - sensitivity: 0.9006 - acc: 0.8122 - val_loss: 0.7138 - val_sensitivity: 0.4170 - val_acc: 0.7669\n",
      "Epoch 72/200\n",
      "6000/6000 [==============================] - 3s 503us/step - loss: 0.6149 - sensitivity: 0.9038 - acc: 0.8170 - val_loss: 0.6894 - val_sensitivity: 0.4395 - val_acc: 0.7798\n",
      "Epoch 73/200\n",
      "6000/6000 [==============================] - 3s 503us/step - loss: 0.6044 - sensitivity: 0.9107 - acc: 0.8108 - val_loss: 0.7302 - val_sensitivity: 0.4000 - val_acc: 0.7724\n",
      "Epoch 74/200\n",
      "6000/6000 [==============================] - 3s 516us/step - loss: 0.6180 - sensitivity: 0.9243 - acc: 0.8093 - val_loss: 0.6860 - val_sensitivity: 0.4294 - val_acc: 0.7816\n",
      "Epoch 75/200\n",
      "6000/6000 [==============================] - 3s 506us/step - loss: 0.5963 - sensitivity: 0.8977 - acc: 0.8163 - val_loss: 0.6958 - val_sensitivity: 0.4074 - val_acc: 0.7810\n",
      "Epoch 76/200\n",
      "6000/6000 [==============================] - 3s 503us/step - loss: 0.5844 - sensitivity: 0.9158 - acc: 0.8220 - val_loss: 0.7103 - val_sensitivity: 0.4053 - val_acc: 0.7914\n",
      "Epoch 77/200\n",
      "6000/6000 [==============================] - 3s 504us/step - loss: 0.5610 - sensitivity: 0.9388 - acc: 0.8275 - val_loss: 0.7337 - val_sensitivity: 0.4129 - val_acc: 0.7939\n",
      "Epoch 78/200\n",
      "6000/6000 [==============================] - 3s 507us/step - loss: 0.5396 - sensitivity: 0.9269 - acc: 0.8375 - val_loss: 0.6989 - val_sensitivity: 0.4286 - val_acc: 0.7840\n",
      "Epoch 79/200\n",
      "6000/6000 [==============================] - 3s 507us/step - loss: 0.5460 - sensitivity: 0.9368 - acc: 0.8333 - val_loss: 0.7219 - val_sensitivity: 0.4055 - val_acc: 0.7896\n",
      "Epoch 80/200\n",
      "6000/6000 [==============================] - 3s 506us/step - loss: 0.5522 - sensitivity: 0.9328 - acc: 0.8332 - val_loss: 0.7300 - val_sensitivity: 0.4166 - val_acc: 0.8080\n",
      "Epoch 81/200\n",
      "6000/6000 [==============================] - 3s 509us/step - loss: 0.5375 - sensitivity: 0.9423 - acc: 0.8375 - val_loss: 0.7374 - val_sensitivity: 0.4025 - val_acc: 0.7871\n",
      "Epoch 82/200\n",
      "6000/6000 [==============================] - 3s 502us/step - loss: 0.5444 - sensitivity: 0.9253 - acc: 0.8452 - val_loss: 0.7407 - val_sensitivity: 0.4086 - val_acc: 0.7963\n",
      "Epoch 83/200\n",
      "6000/6000 [==============================] - 3s 505us/step - loss: 0.5781 - sensitivity: 0.9289 - acc: 0.8255 - val_loss: 0.7557 - val_sensitivity: 0.4031 - val_acc: 0.7816\n",
      "Epoch 84/200\n",
      "6000/6000 [==============================] - 3s 533us/step - loss: 0.5363 - sensitivity: 0.9338 - acc: 0.8412 - val_loss: 0.7148 - val_sensitivity: 0.4458 - val_acc: 0.7626\n",
      "Epoch 85/200\n",
      "6000/6000 [==============================] - 3s 504us/step - loss: 0.5475 - sensitivity: 0.9253 - acc: 0.8397 - val_loss: 0.7355 - val_sensitivity: 0.4200 - val_acc: 0.8074\n",
      "Epoch 86/200\n",
      "6000/6000 [==============================] - 3s 499us/step - loss: 0.5167 - sensitivity: 0.9383 - acc: 0.8470 - val_loss: 0.7869 - val_sensitivity: 0.3883 - val_acc: 0.7810\n",
      "Epoch 87/200\n",
      "6000/6000 [==============================] - 3s 497us/step - loss: 0.5240 - sensitivity: 0.9415 - acc: 0.8533 - val_loss: 0.7378 - val_sensitivity: 0.4335 - val_acc: 0.7969\n",
      "Epoch 88/200\n",
      "6000/6000 [==============================] - 3s 501us/step - loss: 0.5389 - sensitivity: 0.9321 - acc: 0.8310 - val_loss: 0.7631 - val_sensitivity: 0.4117 - val_acc: 0.7810\n",
      "Epoch 89/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 3s 521us/step - loss: 0.5494 - sensitivity: 0.9053 - acc: 0.8425 - val_loss: 0.7209 - val_sensitivity: 0.4356 - val_acc: 0.7883\n",
      "Epoch 90/200\n",
      "6000/6000 [==============================] - 3s 511us/step - loss: 0.5043 - sensitivity: 0.9282 - acc: 0.8470 - val_loss: 0.7700 - val_sensitivity: 0.4237 - val_acc: 0.8123\n",
      "Epoch 91/200\n",
      "6000/6000 [==============================] - 3s 496us/step - loss: 0.5000 - sensitivity: 0.9501 - acc: 0.8462 - val_loss: 0.8144 - val_sensitivity: 0.3830 - val_acc: 0.8055\n",
      "Epoch 92/200\n",
      "6000/6000 [==============================] - 3s 510us/step - loss: 0.5073 - sensitivity: 0.9179 - acc: 0.8490 - val_loss: 0.7679 - val_sensitivity: 0.4178 - val_acc: 0.8074\n",
      "Epoch 93/200\n",
      "6000/6000 [==============================] - 3s 501us/step - loss: 0.4868 - sensitivity: 0.9490 - acc: 0.8630 - val_loss: 0.7659 - val_sensitivity: 0.4227 - val_acc: 0.7963\n",
      "Epoch 94/200\n",
      "6000/6000 [==============================] - 3s 518us/step - loss: 0.4833 - sensitivity: 0.9451 - acc: 0.8568 - val_loss: 0.8067 - val_sensitivity: 0.4067 - val_acc: 0.8117\n",
      "Epoch 95/200\n",
      "6000/6000 [==============================] - 3s 503us/step - loss: 0.4634 - sensitivity: 0.9454 - acc: 0.8708 - val_loss: 0.7367 - val_sensitivity: 0.4307 - val_acc: 0.7804\n",
      "Epoch 96/200\n",
      "6000/6000 [==============================] - 3s 543us/step - loss: 0.6430 - sensitivity: 0.8666 - acc: 0.8138 - val_loss: 0.8265 - val_sensitivity: 0.3781 - val_acc: 0.7798\n",
      "Epoch 97/200\n",
      "6000/6000 [==============================] - 3s 522us/step - loss: 0.5589 - sensitivity: 0.9139 - acc: 0.8347 - val_loss: 0.8230 - val_sensitivity: 0.3646 - val_acc: 0.7509\n",
      "Epoch 98/200\n",
      "6000/6000 [==============================] - 3s 569us/step - loss: 0.5243 - sensitivity: 0.9260 - acc: 0.8480 - val_loss: 0.7880 - val_sensitivity: 0.4092 - val_acc: 0.8031\n",
      "Epoch 99/200\n",
      "6000/6000 [==============================] - 3s 558us/step - loss: 0.4513 - sensitivity: 0.9572 - acc: 0.8675 - val_loss: 0.8245 - val_sensitivity: 0.4074 - val_acc: 0.7957\n",
      "Epoch 100/200\n",
      "6000/6000 [==============================] - 3s 509us/step - loss: 0.4334 - sensitivity: 0.9646 - acc: 0.8762 - val_loss: 0.8285 - val_sensitivity: 0.4135 - val_acc: 0.8264\n",
      "Epoch 101/200\n",
      "6000/6000 [==============================] - 3s 506us/step - loss: 0.4688 - sensitivity: 0.9357 - acc: 0.8615 - val_loss: 0.8205 - val_sensitivity: 0.3912 - val_acc: 0.7914\n",
      "Epoch 102/200\n",
      "6000/6000 [==============================] - 3s 508us/step - loss: 0.4570 - sensitivity: 0.9596 - acc: 0.8660 - val_loss: 0.8299 - val_sensitivity: 0.4221 - val_acc: 0.8209\n",
      "Epoch 103/200\n",
      "6000/6000 [==============================] - 3s 507us/step - loss: 0.4359 - sensitivity: 0.9502 - acc: 0.8767 - val_loss: 0.8081 - val_sensitivity: 0.4252 - val_acc: 0.7982\n",
      "Epoch 104/200\n",
      "6000/6000 [==============================] - 3s 508us/step - loss: 0.4046 - sensitivity: 0.9744 - acc: 0.8852 - val_loss: 0.8473 - val_sensitivity: 0.4190 - val_acc: 0.8227\n",
      "Epoch 105/200\n",
      "6000/6000 [==============================] - 3s 506us/step - loss: 0.3973 - sensitivity: 0.9648 - acc: 0.8828 - val_loss: 0.8246 - val_sensitivity: 0.4215 - val_acc: 0.8294\n",
      "Epoch 106/200\n",
      "6000/6000 [==============================] - 3s 505us/step - loss: 0.4089 - sensitivity: 0.9546 - acc: 0.8837 - val_loss: 0.8598 - val_sensitivity: 0.4117 - val_acc: 0.8196\n",
      "Epoch 107/200\n",
      "6000/6000 [==============================] - 3s 535us/step - loss: 0.4243 - sensitivity: 0.9556 - acc: 0.8728 - val_loss: 0.8543 - val_sensitivity: 0.4080 - val_acc: 0.8049\n",
      "Epoch 108/200\n",
      "6000/6000 [==============================] - 3s 533us/step - loss: 0.4278 - sensitivity: 0.9494 - acc: 0.8792 - val_loss: 0.8294 - val_sensitivity: 0.4104 - val_acc: 0.8141\n",
      "Epoch 109/200\n",
      "6000/6000 [==============================] - 3s 540us/step - loss: 0.4006 - sensitivity: 0.9589 - acc: 0.8867 - val_loss: 0.8453 - val_sensitivity: 0.4245 - val_acc: 0.8288\n",
      "Epoch 110/200\n",
      "6000/6000 [==============================] - 3s 509us/step - loss: 0.4229 - sensitivity: 0.9553 - acc: 0.8838 - val_loss: 0.8566 - val_sensitivity: 0.3990 - val_acc: 0.8092\n",
      "Epoch 111/200\n",
      "6000/6000 [==============================] - 3s 499us/step - loss: 0.5086 - sensitivity: 0.9272 - acc: 0.8557 - val_loss: 0.8303 - val_sensitivity: 0.4000 - val_acc: 0.7883\n",
      "Epoch 112/200\n",
      "6000/6000 [==============================] - 3s 505us/step - loss: 0.4574 - sensitivity: 0.9415 - acc: 0.8692 - val_loss: 0.8217 - val_sensitivity: 0.4252 - val_acc: 0.8098\n",
      "Epoch 113/200\n",
      "6000/6000 [==============================] - 3s 502us/step - loss: 0.4267 - sensitivity: 0.9468 - acc: 0.8818 - val_loss: 0.8581 - val_sensitivity: 0.4149 - val_acc: 0.8209\n",
      "Epoch 114/200\n",
      "6000/6000 [==============================] - 3s 505us/step - loss: 0.4052 - sensitivity: 0.9669 - acc: 0.8870 - val_loss: 0.8369 - val_sensitivity: 0.4258 - val_acc: 0.8172\n",
      "Epoch 115/200\n",
      "6000/6000 [==============================] - 3s 507us/step - loss: 0.3992 - sensitivity: 0.9581 - acc: 0.8853 - val_loss: 0.8586 - val_sensitivity: 0.4155 - val_acc: 0.8098\n",
      "Epoch 116/200\n",
      "6000/6000 [==============================] - 3s 511us/step - loss: 0.4521 - sensitivity: 0.9437 - acc: 0.8692 - val_loss: 0.8588 - val_sensitivity: 0.4010 - val_acc: 0.7988\n",
      "Epoch 117/200\n",
      "6000/6000 [==============================] - 3s 549us/step - loss: 0.4025 - sensitivity: 0.9544 - acc: 0.8887 - val_loss: 0.8394 - val_sensitivity: 0.4282 - val_acc: 0.8160\n",
      "Epoch 118/200\n",
      "6000/6000 [==============================] - 3s 509us/step - loss: 0.3904 - sensitivity: 0.9540 - acc: 0.8897 - val_loss: 0.8441 - val_sensitivity: 0.4258 - val_acc: 0.8252\n",
      "Epoch 119/200\n",
      "6000/6000 [==============================] - 3s 508us/step - loss: 0.3799 - sensitivity: 0.9679 - acc: 0.8915 - val_loss: 0.8880 - val_sensitivity: 0.4147 - val_acc: 0.8337\n",
      "Epoch 120/200\n",
      "6000/6000 [==============================] - 3s 528us/step - loss: 0.3528 - sensitivity: 0.9684 - acc: 0.9000 - val_loss: 0.9047 - val_sensitivity: 0.4178 - val_acc: 0.8350\n",
      "Epoch 121/200\n",
      "6000/6000 [==============================] - 3s 498us/step - loss: 0.3498 - sensitivity: 0.9663 - acc: 0.8993 - val_loss: 0.9257 - val_sensitivity: 0.4094 - val_acc: 0.8245\n",
      "Epoch 122/200\n",
      "6000/6000 [==============================] - 3s 499us/step - loss: 0.3541 - sensitivity: 0.9632 - acc: 0.9027 - val_loss: 0.8722 - val_sensitivity: 0.4168 - val_acc: 0.8294\n",
      "Epoch 123/200\n",
      "6000/6000 [==============================] - 3s 501us/step - loss: 0.3826 - sensitivity: 0.9621 - acc: 0.8912 - val_loss: 0.9391 - val_sensitivity: 0.4184 - val_acc: 0.8380\n",
      "Epoch 124/200\n",
      "6000/6000 [==============================] - 3s 531us/step - loss: 0.3523 - sensitivity: 0.9576 - acc: 0.9003 - val_loss: 0.9128 - val_sensitivity: 0.4184 - val_acc: 0.8294\n",
      "Epoch 125/200\n",
      "6000/6000 [==============================] - 3s 525us/step - loss: 0.3702 - sensitivity: 0.9639 - acc: 0.8975 - val_loss: 0.9894 - val_sensitivity: 0.3984 - val_acc: 0.8350\n",
      "Epoch 126/200\n",
      "6000/6000 [==============================] - 3s 544us/step - loss: 0.3476 - sensitivity: 0.9734 - acc: 0.9015 - val_loss: 0.9850 - val_sensitivity: 0.4061 - val_acc: 0.8141\n",
      "Epoch 127/200\n",
      "6000/6000 [==============================] - 3s 546us/step - loss: 0.3866 - sensitivity: 0.9512 - acc: 0.8952 - val_loss: 0.8977 - val_sensitivity: 0.4110 - val_acc: 0.8006\n",
      "Epoch 128/200\n",
      "6000/6000 [==============================] - 3s 499us/step - loss: 0.3961 - sensitivity: 0.9460 - acc: 0.8902 - val_loss: 0.9278 - val_sensitivity: 0.4074 - val_acc: 0.8117\n",
      "Epoch 129/200\n",
      "6000/6000 [==============================] - 3s 495us/step - loss: 0.4278 - sensitivity: 0.9415 - acc: 0.8807 - val_loss: 0.9188 - val_sensitivity: 0.4178 - val_acc: 0.8202\n",
      "Epoch 130/200\n",
      "6000/6000 [==============================] - 3s 501us/step - loss: 0.6020 - sensitivity: 0.8963 - acc: 0.8372 - val_loss: 0.8936 - val_sensitivity: 0.4037 - val_acc: 0.7951\n",
      "Epoch 131/200\n",
      "6000/6000 [==============================] - 3s 502us/step - loss: 0.5499 - sensitivity: 0.8899 - acc: 0.8475 - val_loss: 0.9141 - val_sensitivity: 0.4014 - val_acc: 0.8074\n",
      "Epoch 132/200\n",
      "6000/6000 [==============================] - 3s 504us/step - loss: 0.3855 - sensitivity: 0.9652 - acc: 0.8907 - val_loss: 0.8848 - val_sensitivity: 0.4082 - val_acc: 0.8319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/200\n",
      "6000/6000 [==============================] - 3s 488us/step - loss: 0.3333 - sensitivity: 0.9746 - acc: 0.9057 - val_loss: 0.9458 - val_sensitivity: 0.4112 - val_acc: 0.8417\n",
      "Epoch 134/200\n",
      "6000/6000 [==============================] - 3s 493us/step - loss: 0.3240 - sensitivity: 0.9799 - acc: 0.9082 - val_loss: 0.9205 - val_sensitivity: 0.4070 - val_acc: 0.8399\n",
      "Epoch 135/200\n",
      "6000/6000 [==============================] - 3s 560us/step - loss: 0.3335 - sensitivity: 0.9763 - acc: 0.9107 - val_loss: 0.9385 - val_sensitivity: 0.4057 - val_acc: 0.8356\n",
      "Epoch 136/200\n",
      "6000/6000 [==============================] - 3s 519us/step - loss: 0.3272 - sensitivity: 0.9738 - acc: 0.9050 - val_loss: 0.8995 - val_sensitivity: 0.4239 - val_acc: 0.8258\n",
      "Epoch 137/200\n",
      "6000/6000 [==============================] - 3s 499us/step - loss: 0.3647 - sensitivity: 0.9695 - acc: 0.8985 - val_loss: 0.9349 - val_sensitivity: 0.4008 - val_acc: 0.8227\n",
      "Epoch 138/200\n",
      "6000/6000 [==============================] - 3s 502us/step - loss: 0.3419 - sensitivity: 0.9620 - acc: 0.9038 - val_loss: 0.9264 - val_sensitivity: 0.4172 - val_acc: 0.8374\n",
      "Epoch 139/200\n",
      "6000/6000 [==============================] - 3s 506us/step - loss: 0.3132 - sensitivity: 0.9783 - acc: 0.9138 - val_loss: 0.9205 - val_sensitivity: 0.4184 - val_acc: 0.8344\n",
      "Epoch 140/200\n",
      "6000/6000 [==============================] - 3s 545us/step - loss: 0.3135 - sensitivity: 0.9818 - acc: 0.9147 - val_loss: 0.9248 - val_sensitivity: 0.4172 - val_acc: 0.8270\n",
      "Epoch 141/200\n",
      "6000/6000 [==============================] - 3s 547us/step - loss: 0.2999 - sensitivity: 0.9834 - acc: 0.9143 - val_loss: 0.9469 - val_sensitivity: 0.4141 - val_acc: 0.8319\n",
      "Epoch 142/200\n",
      "6000/6000 [==============================] - 3s 527us/step - loss: 0.2851 - sensitivity: 0.9917 - acc: 0.9207 - val_loss: 0.9581 - val_sensitivity: 0.4135 - val_acc: 0.8288\n",
      "Epoch 143/200\n",
      "6000/6000 [==============================] - 3s 567us/step - loss: 0.2938 - sensitivity: 0.9805 - acc: 0.9203 - val_loss: 0.9987 - val_sensitivity: 0.4141 - val_acc: 0.8436\n",
      "Epoch 144/200\n",
      "6000/6000 [==============================] - 3s 569us/step - loss: 0.3107 - sensitivity: 0.9718 - acc: 0.9148 - val_loss: 0.9567 - val_sensitivity: 0.4137 - val_acc: 0.8337\n",
      "Epoch 145/200\n",
      "6000/6000 [==============================] - 3s 501us/step - loss: 0.2968 - sensitivity: 0.9809 - acc: 0.9183 - val_loss: 0.9559 - val_sensitivity: 0.4065 - val_acc: 0.8374\n",
      "Epoch 146/200\n",
      "6000/6000 [==============================] - 3s 509us/step - loss: 0.3042 - sensitivity: 0.9672 - acc: 0.9158 - val_loss: 0.9504 - val_sensitivity: 0.4137 - val_acc: 0.8350\n",
      "Epoch 147/200\n",
      "6000/6000 [==============================] - 3s 504us/step - loss: 0.3233 - sensitivity: 0.9634 - acc: 0.9115 - val_loss: 0.9714 - val_sensitivity: 0.4082 - val_acc: 0.8276\n",
      "Epoch 148/200\n",
      "6000/6000 [==============================] - 3s 496us/step - loss: 0.3490 - sensitivity: 0.9570 - acc: 0.9023 - val_loss: 1.1556 - val_sensitivity: 0.3456 - val_acc: 0.7436\n",
      "Epoch 149/200\n",
      "6000/6000 [==============================] - 3s 518us/step - loss: 0.6079 - sensitivity: 0.9161 - acc: 0.8497 - val_loss: 0.9200 - val_sensitivity: 0.3744 - val_acc: 0.7748\n",
      "Epoch 150/200\n",
      "6000/6000 [==============================] - 3s 516us/step - loss: 0.4892 - sensitivity: 0.9395 - acc: 0.8653 - val_loss: 0.9532 - val_sensitivity: 0.3941 - val_acc: 0.8147\n",
      "Epoch 151/200\n",
      "6000/6000 [==============================] - 3s 497us/step - loss: 0.3573 - sensitivity: 0.9619 - acc: 0.8978 - val_loss: 0.9184 - val_sensitivity: 0.4162 - val_acc: 0.8442\n",
      "Epoch 152/200\n",
      "6000/6000 [==============================] - 4s 599us/step - loss: 0.2744 - sensitivity: 0.9889 - acc: 0.9260 - val_loss: 0.9697 - val_sensitivity: 0.4082 - val_acc: 0.8387\n",
      "Epoch 153/200\n",
      "6000/6000 [==============================] - 3s 532us/step - loss: 0.3108 - sensitivity: 0.9684 - acc: 0.9162 - val_loss: 0.9803 - val_sensitivity: 0.3975 - val_acc: 0.8307\n",
      "Epoch 154/200\n",
      "6000/6000 [==============================] - 3s 532us/step - loss: 0.3534 - sensitivity: 0.9644 - acc: 0.9065 - val_loss: 0.9982 - val_sensitivity: 0.4014 - val_acc: 0.8356\n",
      "Epoch 155/200\n",
      "6000/6000 [==============================] - 3s 522us/step - loss: 0.3036 - sensitivity: 0.9741 - acc: 0.9112 - val_loss: 0.9905 - val_sensitivity: 0.4131 - val_acc: 0.8436\n",
      "Epoch 156/200\n",
      "6000/6000 [==============================] - 3s 516us/step - loss: 0.2701 - sensitivity: 0.9851 - acc: 0.9243 - val_loss: 0.9910 - val_sensitivity: 0.4137 - val_acc: 0.8429\n",
      "Epoch 157/200\n",
      "6000/6000 [==============================] - 3s 552us/step - loss: 0.2775 - sensitivity: 0.9879 - acc: 0.9210 - val_loss: 1.0528 - val_sensitivity: 0.3953 - val_acc: 0.8393\n",
      "Epoch 158/200\n",
      "6000/6000 [==============================] - 3s 531us/step - loss: 0.2834 - sensitivity: 0.9741 - acc: 0.9240 - val_loss: 1.0230 - val_sensitivity: 0.4045 - val_acc: 0.8454\n",
      "Epoch 159/200\n",
      "6000/6000 [==============================] - 3s 507us/step - loss: 0.2687 - sensitivity: 0.9831 - acc: 0.9255 - val_loss: 0.9819 - val_sensitivity: 0.4131 - val_acc: 0.8350\n",
      "Epoch 160/200\n",
      "6000/6000 [==============================] - 3s 555us/step - loss: 0.2570 - sensitivity: 0.9881 - acc: 0.9287 - val_loss: 1.0386 - val_sensitivity: 0.4063 - val_acc: 0.8368\n",
      "Epoch 161/200\n",
      "6000/6000 [==============================] - 3s 546us/step - loss: 0.2731 - sensitivity: 0.9857 - acc: 0.9232 - val_loss: 1.0152 - val_sensitivity: 0.4100 - val_acc: 0.8380\n",
      "Epoch 162/200\n",
      "6000/6000 [==============================] - 3s 502us/step - loss: 0.2530 - sensitivity: 0.9822 - acc: 0.9302 - val_loss: 1.0251 - val_sensitivity: 0.4106 - val_acc: 0.8448\n",
      "Epoch 163/200\n",
      "6000/6000 [==============================] - 3s 554us/step - loss: 0.2546 - sensitivity: 0.9807 - acc: 0.9298 - val_loss: 0.9941 - val_sensitivity: 0.4125 - val_acc: 0.8442\n",
      "Epoch 164/200\n",
      "6000/6000 [==============================] - 3s 515us/step - loss: 0.2281 - sensitivity: 0.9962 - acc: 0.9367 - val_loss: 1.0575 - val_sensitivity: 0.4063 - val_acc: 0.8503\n",
      "Epoch 165/200\n",
      "6000/6000 [==============================] - 3s 508us/step - loss: 0.3144 - sensitivity: 0.9665 - acc: 0.9188 - val_loss: 0.9867 - val_sensitivity: 0.4027 - val_acc: 0.8301\n",
      "Epoch 166/200\n",
      "6000/6000 [==============================] - 3s 509us/step - loss: 0.3034 - sensitivity: 0.9798 - acc: 0.9157 - val_loss: 1.0074 - val_sensitivity: 0.4094 - val_acc: 0.8362\n",
      "Epoch 167/200\n",
      "6000/6000 [==============================] - 3s 536us/step - loss: 0.2449 - sensitivity: 0.9935 - acc: 0.9328 - val_loss: 1.0150 - val_sensitivity: 0.4131 - val_acc: 0.8393\n",
      "Epoch 168/200\n",
      "6000/6000 [==============================] - 4s 609us/step - loss: 0.2427 - sensitivity: 0.9870 - acc: 0.9342 - val_loss: 1.0346 - val_sensitivity: 0.4076 - val_acc: 0.8460\n",
      "Epoch 169/200\n",
      "6000/6000 [==============================] - 3s 565us/step - loss: 0.2477 - sensitivity: 0.9905 - acc: 0.9353 - val_loss: 1.0451 - val_sensitivity: 0.4076 - val_acc: 0.8393\n",
      "Epoch 170/200\n",
      "6000/6000 [==============================] - 3s 526us/step - loss: 0.2240 - sensitivity: 0.9916 - acc: 0.9368 - val_loss: 1.0859 - val_sensitivity: 0.4045 - val_acc: 0.8491\n",
      "Epoch 171/200\n",
      "6000/6000 [==============================] - 3s 489us/step - loss: 0.2398 - sensitivity: 0.9833 - acc: 0.9360 - val_loss: 1.0791 - val_sensitivity: 0.4041 - val_acc: 0.8405\n",
      "Epoch 172/200\n",
      "6000/6000 [==============================] - 4s 595us/step - loss: 0.2818 - sensitivity: 0.9657 - acc: 0.9270 - val_loss: 1.0420 - val_sensitivity: 0.4070 - val_acc: 0.8405\n",
      "Epoch 173/200\n",
      "6000/6000 [==============================] - 3s 511us/step - loss: 0.2850 - sensitivity: 0.9796 - acc: 0.9233 - val_loss: 1.0350 - val_sensitivity: 0.4123 - val_acc: 0.8282\n",
      "Epoch 174/200\n",
      "6000/6000 [==============================] - 3s 512us/step - loss: 0.2728 - sensitivity: 0.9606 - acc: 0.9252 - val_loss: 1.0369 - val_sensitivity: 0.4088 - val_acc: 0.8350\n",
      "Epoch 175/200\n",
      "6000/6000 [==============================] - 3s 511us/step - loss: 0.2245 - sensitivity: 0.9940 - acc: 0.9420 - val_loss: 1.0656 - val_sensitivity: 0.4082 - val_acc: 0.8448\n",
      "Epoch 176/200\n",
      "6000/6000 [==============================] - 3s 502us/step - loss: 0.2635 - sensitivity: 0.9792 - acc: 0.9313 - val_loss: 0.9958 - val_sensitivity: 0.4168 - val_acc: 0.8252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/200\n",
      "6000/6000 [==============================] - 3s 505us/step - loss: 0.2534 - sensitivity: 0.9672 - acc: 0.9252 - val_loss: 1.0373 - val_sensitivity: 0.4082 - val_acc: 0.8399\n",
      "Epoch 178/200\n",
      "6000/6000 [==============================] - 3s 496us/step - loss: 0.2249 - sensitivity: 0.9887 - acc: 0.9413 - val_loss: 1.0609 - val_sensitivity: 0.4106 - val_acc: 0.8405\n",
      "Epoch 179/200\n",
      "6000/6000 [==============================] - 3s 492us/step - loss: 0.2428 - sensitivity: 0.9854 - acc: 0.9335 - val_loss: 1.0533 - val_sensitivity: 0.4063 - val_acc: 0.8399\n",
      "Epoch 180/200\n",
      "6000/6000 [==============================] - 3s 500us/step - loss: 0.2554 - sensitivity: 0.9788 - acc: 0.9307 - val_loss: 1.0164 - val_sensitivity: 0.4168 - val_acc: 0.8337\n",
      "Epoch 181/200\n",
      "6000/6000 [==============================] - 3s 499us/step - loss: 0.2679 - sensitivity: 0.9526 - acc: 0.9265 - val_loss: 1.0283 - val_sensitivity: 0.4070 - val_acc: 0.8325\n",
      "Epoch 182/200\n",
      "6000/6000 [==============================] - 3s 512us/step - loss: 0.3142 - sensitivity: 0.9576 - acc: 0.9172 - val_loss: 1.0564 - val_sensitivity: 0.4074 - val_acc: 0.8239\n",
      "Epoch 183/200\n",
      "6000/6000 [==============================] - 3s 510us/step - loss: 0.2924 - sensitivity: 0.9656 - acc: 0.9222 - val_loss: 1.0130 - val_sensitivity: 0.4125 - val_acc: 0.8258\n",
      "Epoch 184/200\n",
      "6000/6000 [==============================] - 3s 509us/step - loss: 0.2759 - sensitivity: 0.9753 - acc: 0.9278 - val_loss: 1.0826 - val_sensitivity: 0.4033 - val_acc: 0.8436\n",
      "Epoch 185/200\n",
      "6000/6000 [==============================] - 3s 519us/step - loss: 0.2925 - sensitivity: 0.9679 - acc: 0.9243 - val_loss: 1.0716 - val_sensitivity: 0.4008 - val_acc: 0.8031\n",
      "Epoch 186/200\n",
      "6000/6000 [==============================] - 3s 504us/step - loss: 0.4492 - sensitivity: 0.9149 - acc: 0.8878 - val_loss: 1.0154 - val_sensitivity: 0.4092 - val_acc: 0.8319\n",
      "Epoch 187/200\n",
      "6000/6000 [==============================] - 3s 514us/step - loss: 0.2552 - sensitivity: 0.9742 - acc: 0.9288 - val_loss: 1.1268 - val_sensitivity: 0.3910 - val_acc: 0.8319\n",
      "Epoch 188/200\n",
      "6000/6000 [==============================] - 3s 541us/step - loss: 0.2566 - sensitivity: 0.9683 - acc: 0.9360 - val_loss: 1.0428 - val_sensitivity: 0.4131 - val_acc: 0.8184\n",
      "Epoch 189/200\n",
      "6000/6000 [==============================] - 3s 552us/step - loss: 0.4414 - sensitivity: 0.9316 - acc: 0.8887 - val_loss: 0.9999 - val_sensitivity: 0.4168 - val_acc: 0.8288\n",
      "Epoch 190/200\n",
      "6000/6000 [==============================] - 4s 596us/step - loss: 0.2721 - sensitivity: 0.9749 - acc: 0.9272 - val_loss: 1.0501 - val_sensitivity: 0.4110 - val_acc: 0.8466\n",
      "Epoch 191/200\n",
      "6000/6000 [==============================] - 3s 580us/step - loss: 0.2052 - sensitivity: 0.9958 - acc: 0.9465 - val_loss: 1.1015 - val_sensitivity: 0.4045 - val_acc: 0.8454\n",
      "Epoch 192/200\n",
      "6000/6000 [==============================] - 3s 513us/step - loss: 0.1836 - sensitivity: 0.9956 - acc: 0.9512 - val_loss: 1.1154 - val_sensitivity: 0.4020 - val_acc: 0.8405\n",
      "Epoch 193/200\n",
      "6000/6000 [==============================] - 3s 509us/step - loss: 0.1862 - sensitivity: 0.9907 - acc: 0.9508 - val_loss: 1.1064 - val_sensitivity: 0.4076 - val_acc: 0.8540\n",
      "Epoch 194/200\n",
      "6000/6000 [==============================] - 3s 515us/step - loss: 0.1789 - sensitivity: 0.9952 - acc: 0.9542 - val_loss: 1.1340 - val_sensitivity: 0.4020 - val_acc: 0.8503\n",
      "Epoch 195/200\n",
      "6000/6000 [==============================] - 3s 524us/step - loss: 0.2472 - sensitivity: 0.9726 - acc: 0.9368 - val_loss: 1.0838 - val_sensitivity: 0.4166 - val_acc: 0.8313\n",
      "Epoch 196/200\n",
      "6000/6000 [==============================] - 3s 507us/step - loss: 0.2311 - sensitivity: 0.9838 - acc: 0.9395 - val_loss: 1.1062 - val_sensitivity: 0.4063 - val_acc: 0.8442\n",
      "Epoch 197/200\n",
      "6000/6000 [==============================] - 3s 509us/step - loss: 0.2161 - sensitivity: 0.9861 - acc: 0.9435 - val_loss: 1.0921 - val_sensitivity: 0.4106 - val_acc: 0.8503\n",
      "Epoch 198/200\n",
      "6000/6000 [==============================] - 3s 519us/step - loss: 0.2204 - sensitivity: 0.9706 - acc: 0.9442 - val_loss: 1.0574 - val_sensitivity: 0.4198 - val_acc: 0.8196\n",
      "Epoch 199/200\n",
      "6000/6000 [==============================] - 3s 525us/step - loss: 0.2601 - sensitivity: 0.9756 - acc: 0.9287 - val_loss: 1.1033 - val_sensitivity: 0.4088 - val_acc: 0.8393\n",
      "Epoch 200/200\n",
      "6000/6000 [==============================] - 3s 509us/step - loss: 0.1914 - sensitivity: 0.9948 - acc: 0.9502 - val_loss: 1.1363 - val_sensitivity: 0.4088 - val_acc: 0.8515\n",
      "0.8414285714285714\n",
      "[[1139  143]\n",
      " [  79   39]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8515337423312883\n",
      "[[750  65]\n",
      " [177 638]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = get_model()\n",
    "early_stopping = EarlyStopping(monitor='val_acc', mode='min', verbose=1, patience = 90)\n",
    "\n",
    "model.fit(train_x, train_y, validation_data=(validation_x, validation_y), \n",
    "    epochs=200, class_weight = {0:1, 1:10}, batch_size=100,shuffle=True)\n",
    "\n",
    "preds_val = model.predict(validation_x, batch_size = 100)\n",
    "val_acc = accuracy_score(preds_val > 0.5, validation_y > 0.5)\n",
    "fpr, tpr, thresholds = roc_curve(validation_y > 0.5, preds_val > 0.5)\n",
    "conf_mat = confusion_matrix(validation_y > 0.5, preds_val > 0.5)\n",
    "print(val_acc)\n",
    "print(conf_mat)\n",
    "plt.plot(fpr, tpr, marker = '.')\n",
    "plt.show()\n",
    "\n",
    "preds_test = model.predict(test_x, batch_size = 100)\n",
    "test_acc = accuracy_score(preds_test > 0.5, test_y > 0.5)\n",
    "fpr, tpr, thresholds = roc_curve(test_y > 0.5, preds_test > 0.5)\n",
    "conf_mat = confusion_matrix(test_y > 0.5, preds_test > 0.5)\n",
    "print(test_acc)\n",
    "print(conf_mat)\n",
    "plt.plot(fpr, tpr, marker = '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
