{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate data preparation\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "import os\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps, n_lap):\n",
    "\tX = list()\n",
    "\tfor i in range(0, len(sequences), n_lap):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x = sequences[i:end_ix, :]\n",
    "\t\tX.append(seq_x)\n",
    "\t\t\n",
    "\treturn array(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout, Activation, LSTM, Lambda\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Reshape\n",
    "from keras.models import Sequential\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Reshape\n",
    "from keras.layers.merge import concatenate, dot\n",
    "from keras.layers.core import Reshape\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0xc696d73c8>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model():\n",
    "    kernel_size = 5\n",
    "    filters = 10 \n",
    "    stride = 4\n",
    "    pool_size = 4\n",
    "    lstm_output_size = 100\n",
    "    \n",
    "    inp = Input(shape = (2, 11, 20, 64, 3))\n",
    "    mat1 = Lambda(lambda x: x[:, 0])(inp)\n",
    "    mat2 = Lambda(lambda x: x[:, 1])(inp)\n",
    "    mat1 = TimeDistributed(Convolution2D(filters = filters, kernel_size = kernel_size, \n",
    "                                         strides = (stride, stride), activation = 'relu'))(mat1)\n",
    "    mat2 = TimeDistributed(Convolution2D(filters = filters, kernel_size = kernel_size, \n",
    "                                         strides = (stride, stride), activation = 'relu'))(mat2)\n",
    "    \n",
    "    mat1 = TimeDistributed(MaxPooling2D(pool_size = pool_size, strides = 2))(mat1)\n",
    "    mat2 = TimeDistributed(MaxPooling2D(pool_size = pool_size, strides = 2))(mat2)\n",
    "    mat1 = TimeDistributed(Dropout(rate = 0.25))(mat1)\n",
    "    mat2 = TimeDistributed(Dropout(rate = 0.25))(mat2)\n",
    "    mat1 = TimeDistributed(Flatten())(mat1)\n",
    "    mat2 = TimeDistributed(Flatten())(mat2)\n",
    "    \n",
    "    mat1 = LSTM(lstm_output_size)(mat1)\n",
    "    mat2 = LSTM(lstm_output_size)(mat2)\n",
    "    \n",
    "    vec3 = dot([mat1, mat2], axes = 1)\n",
    "    vec4 = concatenate([mat1, mat2, vec3])\n",
    "\n",
    "    vec4 = Dense(100,  activation='relu')(vec4)\n",
    "    vec4 = Dropout(rate=0.25)(vec4)\n",
    "    preds = Dense(1, activation='sigmoid')(vec4)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=preds)\n",
    "    \n",
    "    opt = Adam(lr=0.001, decay=1e-6)\n",
    "    model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mse'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "get_model()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.load('train_set_score.npy')\n",
    "\n",
    "val_set = np.load('val_set_score.npy')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 2, 11, 20, 64, 3)\n",
      "(6000,)\n",
      "(1400, 2, 11, 20, 64, 3)\n",
      "(1400,)\n",
      "(2400, 2, 11, 20, 64, 3)\n",
      "(2400, 3)\n"
     ]
    }
   ],
   "source": [
    "n_steps = 20 #10seconds\n",
    "n_lap = 10 #5seconds\n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "for i in range(6000):\n",
    "    image1 = split_sequences(np.transpose(train_set[i][0], (1,0,2)), n_steps, n_lap)\n",
    "    image2 = split_sequences(np.transpose(train_set[i][1], (1,0,2)), n_steps, n_lap)\n",
    "    y = train_set[i][2]\n",
    "    train_x.append(np.array([image1, image2]))\n",
    "    train_y.append(y)\n",
    "    \n",
    "train_x = np.array(train_x)\n",
    "print(train_x.shape)\n",
    "train_y = np.array(train_y)\n",
    "print(train_y.shape)\n",
    "\n",
    "validation_x = []\n",
    "validation_y = []\n",
    "for i in range(6000, 7400):\n",
    "    image1 = split_sequences(np.transpose(train_set[i][0], (1,0,2)), n_steps, n_lap)\n",
    "    image2 = split_sequences(np.transpose(train_set[i][1], (1,0,2)), n_steps, n_lap)\n",
    "    y = train_set[i][2]\n",
    "    validation_x.append(np.array([image1, image2]))\n",
    "    validation_y.append(y)\n",
    "    \n",
    "validation_x = np.array(validation_x)\n",
    "print(validation_x.shape)\n",
    "validation_y = np.array(validation_y)\n",
    "print(validation_y.shape)\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "for i in range(2400):\n",
    "    image1 = split_sequences(np.transpose(train_set[i][0], (1,0,2)), n_steps, n_lap)\n",
    "    image2 = split_sequences(np.transpose(train_set[i][1], (1,0,2)), n_steps, n_lap)\n",
    "    y = train_set[i][2]\n",
    "    test_x.append(np.array([image1, image2]))\n",
    "    test_y.append(y)\n",
    "    \n",
    "test_x = np.array(test_x)\n",
    "print(test_x.shape)\n",
    "test_y = np.array(test_y)\n",
    "print(test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1400 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 15s 2ms/step - loss: 0.0639 - mean_squared_error: 0.0639 - val_loss: 0.0617 - val_mean_squared_error: 0.0617\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 6s 929us/step - loss: 0.0611 - mean_squared_error: 0.0611 - val_loss: 0.0567 - val_mean_squared_error: 0.0567\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 6s 943us/step - loss: 0.0614 - mean_squared_error: 0.0614 - val_loss: 0.0562 - val_mean_squared_error: 0.0562\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 6s 931us/step - loss: 0.0606 - mean_squared_error: 0.0606 - val_loss: 0.0559 - val_mean_squared_error: 0.0559\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 6s 930us/step - loss: 0.0603 - mean_squared_error: 0.0603 - val_loss: 0.0559 - val_mean_squared_error: 0.0559\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 6s 946us/step - loss: 0.0605 - mean_squared_error: 0.0605 - val_loss: 0.0559 - val_mean_squared_error: 0.0559\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 6s 962us/step - loss: 0.0603 - mean_squared_error: 0.0603 - val_loss: 0.0561 - val_mean_squared_error: 0.0561\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 6s 955us/step - loss: 0.0608 - mean_squared_error: 0.0608 - val_loss: 0.0559 - val_mean_squared_error: 0.0559\n",
      "Epoch 9/100\n",
      "6000/6000 [==============================] - 6s 953us/step - loss: 0.0605 - mean_squared_error: 0.0605 - val_loss: 0.0575 - val_mean_squared_error: 0.0575\n",
      "Epoch 10/100\n",
      "6000/6000 [==============================] - 6s 960us/step - loss: 0.0603 - mean_squared_error: 0.0603 - val_loss: 0.0558 - val_mean_squared_error: 0.0558\n",
      "Epoch 11/100\n",
      "6000/6000 [==============================] - 6s 957us/step - loss: 0.0606 - mean_squared_error: 0.0606 - val_loss: 0.0560 - val_mean_squared_error: 0.0560\n",
      "Epoch 12/100\n",
      "6000/6000 [==============================] - 6s 959us/step - loss: 0.0603 - mean_squared_error: 0.0603 - val_loss: 0.0559 - val_mean_squared_error: 0.0559\n",
      "Epoch 13/100\n",
      "6000/6000 [==============================] - 6s 965us/step - loss: 0.0599 - mean_squared_error: 0.0599 - val_loss: 0.0559 - val_mean_squared_error: 0.0559\n",
      "Epoch 14/100\n",
      "6000/6000 [==============================] - 6s 973us/step - loss: 0.0602 - mean_squared_error: 0.0602 - val_loss: 0.0566 - val_mean_squared_error: 0.0566\n",
      "Epoch 15/100\n",
      "6000/6000 [==============================] - 6s 991us/step - loss: 0.0605 - mean_squared_error: 0.0605 - val_loss: 0.0567 - val_mean_squared_error: 0.0567\n",
      "Epoch 16/100\n",
      "6000/6000 [==============================] - 6s 946us/step - loss: 0.0605 - mean_squared_error: 0.0605 - val_loss: 0.0559 - val_mean_squared_error: 0.0559\n",
      "Epoch 17/100\n",
      "6000/6000 [==============================] - 6s 950us/step - loss: 0.0601 - mean_squared_error: 0.0601 - val_loss: 0.0562 - val_mean_squared_error: 0.0562\n",
      "Epoch 18/100\n",
      "6000/6000 [==============================] - 6s 953us/step - loss: 0.0601 - mean_squared_error: 0.0601 - val_loss: 0.0561 - val_mean_squared_error: 0.0561\n",
      "Epoch 19/100\n",
      "6000/6000 [==============================] - 6s 963us/step - loss: 0.0601 - mean_squared_error: 0.0601 - val_loss: 0.0559 - val_mean_squared_error: 0.0559\n",
      "Epoch 20/100\n",
      "6000/6000 [==============================] - 6s 951us/step - loss: 0.0600 - mean_squared_error: 0.0600 - val_loss: 0.0559 - val_mean_squared_error: 0.0559\n",
      "Epoch 00020: early stopping\n",
      "0.05584337026954368 0.9157142857142857\n",
      "0.054691485653049605 0.92\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 10)\n",
    "\n",
    "model.fit(train_x, train_y, validation_data=(validation_x, validation_y), \n",
    "    epochs=100, batch_size=100, shuffle=True, callbacks=[early_stopping])\n",
    "\n",
    "preds_val = model.predict(validation_x, batch_size = 100)\n",
    "val_mse = np.mean((preds_val - validation_y)**2)\n",
    "val_acc = accuracy_score(preds_val > 0.5, validation_y > 0.5)\n",
    "print(val_mse, val_acc)\n",
    "\n",
    "preds_test = model.predict(test_x, batch_size = 100)\n",
    "test_mse = np.mean((preds_test - test_y)**2)\n",
    "test_acc = accuracy_score(preds_test > 0.5, test_y > 0.5)\n",
    "print(test_mse, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
